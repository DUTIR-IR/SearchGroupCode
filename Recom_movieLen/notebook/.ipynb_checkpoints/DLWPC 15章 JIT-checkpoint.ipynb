{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-13T14:46:07.591772Z",
     "start_time": "2020-10-13T14:46:06.986350Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import re\n",
    "import copy\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-13T14:46:07.597621Z",
     "start_time": "2020-10-13T14:46:07.593588Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-13T14:46:08.005293Z",
     "start_time": "2020-10-13T14:46:07.599344Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-13T14:46:08.245774Z",
     "start_time": "2020-10-13T14:46:08.007318Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-13T14:46:08.260519Z",
     "start_time": "2020-10-13T14:46:08.247272Z"
    }
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    DEVICE = 'cuda:0'\n",
    "else:\n",
    "    DEVICE = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_usr=  943\n",
    "total_item = 1682"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_num = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 复杂模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-13T14:46:08.341135Z",
     "start_time": "2020-10-13T14:46:08.329253Z"
    }
   },
   "outputs": [],
   "source": [
    "class RecModel(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 usr_num:int,\n",
    "                 item_num:int,\n",
    "                 emb_usr_size:int,\n",
    "                 emb_item_size:int,\n",
    "                 hidden_size:int):\n",
    "        '''\n",
    "        usr_num 和 item_num 为对应用户和物品的总数\n",
    "        emb_size设定用户和物品的隐变量维度\n",
    "        hidden_size为交互时的维度\n",
    "        '''\n",
    "        super(RecModel,self).__init__()\n",
    "        self.usr_num = usr_num\n",
    "        self.item_num = item_num\n",
    "        self.emb_usr_size = emb_usr_size\n",
    "        self.emb_item_size = emb_item_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # 查看显卡设备是否可用 \n",
    "        if torch.cuda.is_available():\n",
    "            self.device = 'cuda:0'\n",
    "        else:\n",
    "            self.device = 'cpu'\n",
    "            \n",
    "        self.UserEmbeddingLayer = torch.nn.Embedding(num_embeddings=self.usr_num,\n",
    "                                                     embedding_dim=self.emb_usr_size)\n",
    "        self.ItemEmbeddingLayer = torch.nn.Embedding(num_embeddings=self.item_num,\n",
    "                                                     embedding_dim=self.emb_item_size)\n",
    "        self.UserFeatureLayer = torch.nn.Linear(in_features=self.emb_usr_size,\n",
    "                                                out_features=self.hidden_size)\n",
    "        self.ItemFeatureLayer = torch.nn.Linear(in_features=self.emb_item_size,\n",
    "                                               out_features=self.hidden_size)\n",
    "        # to device\n",
    "        self.UserEmbeddingLayer.to(self.device)\n",
    "        self.UserFeatureLayer.to(self.device)\n",
    "        self.ItemEmbeddingLayer.to(self.device)\n",
    "        self.ItemFeatureLayer.to(self.device)\n",
    "        \n",
    "    def forward(self,uid_batch,iid_batch):\n",
    "        '''输入一个batch的usr和item进行交互\n",
    "        '''\n",
    "        u_batch_tensor = torch.tensor(uid_batch)\n",
    "        i_batch_tensor = torch.tensor(iid_batch)\n",
    "        # 装入设备\n",
    "        u_batch_tensor = u_batch_tensor.to(self.device)\n",
    "        i_batch_tensor = i_batch_tensor.to(self.device)\n",
    "        # 嵌入 向量化\n",
    "        u_emb_tensor = self.UserEmbeddingLayer(u_batch_tensor)\n",
    "        i_emb_tensor = self.ItemEmbeddingLayer(i_batch_tensor)\n",
    "        # 特征抽取 和 非线性化\n",
    "        u_feature = self.UserFeatureLayer(u_emb_tensor)\n",
    "        i_feature = self.ItemFeatureLayer(i_emb_tensor)\n",
    "        \n",
    "        u_feature = torch.relu(u_feature)\n",
    "        i_feature = torch.relu(i_feature)\n",
    "        \n",
    "        batch_size = u_feature.shape[0]\n",
    "        u_feature = u_feature.reshape(batch_size,1,self.hidden_size)\n",
    "        i_feature = i_feature.reshape(batch_size,self.hidden_size,1)\n",
    "        \n",
    "        output = torch.bmm(u_feature,i_feature)\n",
    "        output = torch.squeeze(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "usr_batch = [random.randint(0,total_usr-1) for i in range(batch_num)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "item_batch = [random.randint(0,total_item-1) for i in range(batch_num)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-13T14:46:08.352609Z",
     "start_time": "2020-10-13T14:46:08.342915Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.2741, 2.4860, 1.9223, 0.8817, 2.1885, 3.9245, 1.4730, 2.1707, 1.5449,\n",
       "        0.3025, 3.0176, 2.0375, 0.9042, 1.3076, 1.9867, 1.3308, 0.6745, 1.6089,\n",
       "        1.1327, 2.3930, 1.3210, 0.4267, 2.8317, 0.8065, 0.7984, 1.9755, 0.6963,\n",
       "        0.8318, 0.4522, 1.1351, 3.8259, 1.7693, 0.3374, 0.5228, 2.8876, 0.7399,\n",
       "        1.8594, 2.0928, 0.8713, 0.6497, 1.3657, 1.1330, 1.3151, 0.8565, 2.2704,\n",
       "        0.8269, 2.8218, 0.6436, 3.0725, 1.6693, 2.2088, 1.3097, 1.0366, 1.3439,\n",
       "        0.0890, 1.2549, 0.3911, 0.6793, 1.4494, 2.9492, 0.3521, 0.0114, 0.9335,\n",
       "        2.4840], grad_fn=<SqueezeBackward0>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn = RecModel(usr_num=total_usr,item_num=total_item,emb_usr_size=50,emb_item_size=150,hidden_size=25)\n",
    "nn(usr_batch,item_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试torchScript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scripted_model = torch.jit.script(nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RecursiveScriptModule(\n",
       "  original_name=RecModelPureTorch\n",
       "  (UserEmbeddingLayer): RecursiveScriptModule(original_name=Embedding)\n",
       "  (ItemEmbeddingLayer): RecursiveScriptModule(original_name=Embedding)\n",
       "  (UserFeatureLayer): RecursiveScriptModule(original_name=Linear)\n",
       "  (ItemFeatureLayer): RecursiveScriptModule(original_name=Linear)\n",
       ")"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scripted_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def forward(self,\n",
      "    uid_batch: Tensor,\n",
      "    iid_batch: Tensor) -> Tensor:\n",
      "  _0 = self.ItemFeatureLayer\n",
      "  _1 = self.UserFeatureLayer\n",
      "  _2 = self.ItemEmbeddingLayer\n",
      "  _3 = self.UserEmbeddingLayer\n",
      "  input = torch.to(uid_batch, dtype=4, layout=0, device=torch.device(\"cpu\"), pin_memory=False, non_blocking=False, copy=False, memory_format=None)\n",
      "  input0 = torch.to(iid_batch, dtype=4, layout=0, device=torch.device(\"cpu\"), pin_memory=False, non_blocking=False, copy=False, memory_format=None)\n",
      "  _4 = (_3).forward(input, )\n",
      "  _5 = (_2).forward(input0, )\n",
      "  _6 = (_1).forward(_4, )\n",
      "  _7 = (_0).forward(_5, )\n",
      "  u_feature = torch.relu(_6)\n",
      "  i_feature = torch.relu(_7)\n",
      "  batch_size = ops.prim.NumToTensor(torch.size(u_feature, 0))\n",
      "  _8 = int(batch_size)\n",
      "  u_feature0 = torch.reshape(u_feature, [int(batch_size), 1, 25])\n",
      "  i_feature0 = torch.reshape(i_feature, [_8, 25, 1])\n",
      "  output = torch.bmm(u_feature0, i_feature0)\n",
      "  return torch.squeeze(output)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(traced_model.code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Type 'Tuple[List[int], List[int]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-120-f08171c734d0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0musr_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mitem_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\tools\\Anaconda3\\lib\\site-packages\\torch\\jit\\__init__.py\u001b[0m in \u001b[0;36mtrace\u001b[1;34m(func, example_inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit)\u001b[0m\n\u001b[0;32m    951\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 953\u001b[1;33m         return trace_module(func, {'forward': example_inputs}, None,\n\u001b[0m\u001b[0;32m    954\u001b[0m                             \u001b[0mcheck_trace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwrap_check_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheck_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    955\u001b[0m                             check_tolerance, strict, _force_outplace, _module_class)\n",
      "\u001b[1;32mC:\\tools\\Anaconda3\\lib\\site-packages\\torch\\jit\\__init__.py\u001b[0m in \u001b[0;36mtrace_module\u001b[1;34m(mod, inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit)\u001b[0m\n\u001b[0;32m   1107\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmod\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mmethod_name\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"forward\"\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1108\u001b[0m             \u001b[0mexample_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexample_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1109\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_c\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_method_from_trace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexample_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar_lookup_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_force_outplace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1110\u001b[0m             \u001b[0mcheck_trace_method\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_c\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Type 'Tuple[List[int], List[int]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced"
     ]
    }
   ],
   "source": [
    "torch.jit.trace(nn,(usr_batch,item_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 修改模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-13T14:46:08.341135Z",
     "start_time": "2020-10-13T14:46:08.329253Z"
    }
   },
   "outputs": [],
   "source": [
    "class RecModelPureTorch(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 usr_num:int,\n",
    "                 item_num:int,\n",
    "                 emb_usr_size:int,\n",
    "                 emb_item_size:int,\n",
    "                 hidden_size:int):\n",
    "        '''\n",
    "        usr_num 和 item_num 为对应用户和物品的总数\n",
    "        emb_size设定用户和物品的隐变量维度\n",
    "        hidden_size为交互时的维度\n",
    "        '''\n",
    "        super(RecModelPureTorch,self).__init__()\n",
    "        self.usr_num = usr_num\n",
    "        self.item_num = item_num\n",
    "        self.emb_usr_size = emb_usr_size\n",
    "        self.emb_item_size = emb_item_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # 查看显卡设备是否可用 \n",
    "        if torch.cuda.is_available():\n",
    "            self.device = 'cuda:0'\n",
    "        else:\n",
    "            self.device = 'cpu'\n",
    "            \n",
    "        self.UserEmbeddingLayer = torch.nn.Embedding(num_embeddings=self.usr_num,\n",
    "                                                     embedding_dim=self.emb_usr_size)\n",
    "        self.ItemEmbeddingLayer = torch.nn.Embedding(num_embeddings=self.item_num,\n",
    "                                                     embedding_dim=self.emb_item_size)\n",
    "        self.UserFeatureLayer = torch.nn.Linear(in_features=self.emb_usr_size,\n",
    "                                                out_features=self.hidden_size)\n",
    "        self.ItemFeatureLayer = torch.nn.Linear(in_features=self.emb_item_size,\n",
    "                                               out_features=self.hidden_size)\n",
    "        # to device\n",
    "        self.UserEmbeddingLayer.to(self.device)\n",
    "        self.UserFeatureLayer.to(self.device)\n",
    "        self.ItemEmbeddingLayer.to(self.device)\n",
    "        self.ItemFeatureLayer.to(self.device)\n",
    "        \n",
    "        \n",
    "    def forward(self,uid_batch,iid_batch):\n",
    "        '''输入一个batch的usr和item进行交互\n",
    "        '''\n",
    "\n",
    "        # 装入设备\n",
    "        u_batch_tensor = uid_batch.to(self.device)\n",
    "        i_batch_tensor = iid_batch.to(self.device)\n",
    "        # 嵌入 向量化\n",
    "        u_emb_tensor = self.UserEmbeddingLayer(u_batch_tensor)\n",
    "        i_emb_tensor = self.ItemEmbeddingLayer(i_batch_tensor)\n",
    "        # 特征抽取 和 非线性化\n",
    "        u_feature = self.UserFeatureLayer(u_emb_tensor)\n",
    "        i_feature = self.ItemFeatureLayer(i_emb_tensor)\n",
    "        \n",
    "        u_feature = torch.relu(u_feature)\n",
    "        i_feature = torch.relu(i_feature)\n",
    "        \n",
    "        batch_size = u_feature.shape[0]\n",
    "        u_feature = u_feature.reshape(batch_size,1,self.hidden_size)\n",
    "        i_feature = i_feature.reshape(batch_size,self.hidden_size,1)\n",
    "        \n",
    "        output = torch.bmm(u_feature,i_feature)\n",
    "        output = torch.squeeze(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "usr_trace_batch = torch.tensor(usr_batch)\n",
    "item_trace_batch = torch.tensor(item_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_pure_torch = RecModelPureTorch(usr_num=total_usr,item_num=total_item,emb_usr_size=50,emb_item_size=150,hidden_size=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "traced_model = torch.jit.trace(nn_pure_torch,(usr_trace_batch,item_trace_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RecModelPureTorch(\n",
       "  original_name=RecModelPureTorch\n",
       "  (UserEmbeddingLayer): Embedding(original_name=Embedding)\n",
       "  (ItemEmbeddingLayer): Embedding(original_name=Embedding)\n",
       "  (UserFeatureLayer): Linear(original_name=Linear)\n",
       "  (ItemFeatureLayer): Linear(original_name=Linear)\n",
       ")"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traced_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def forward(self,\n",
      "    uid_batch: Tensor,\n",
      "    iid_batch: Tensor) -> Tensor:\n",
      "  _0 = self.ItemFeatureLayer\n",
      "  _1 = self.UserFeatureLayer\n",
      "  _2 = self.ItemEmbeddingLayer\n",
      "  _3 = self.UserEmbeddingLayer\n",
      "  input = torch.to(uid_batch, dtype=4, layout=0, device=torch.device(\"cpu\"), pin_memory=False, non_blocking=False, copy=False, memory_format=None)\n",
      "  input0 = torch.to(iid_batch, dtype=4, layout=0, device=torch.device(\"cpu\"), pin_memory=False, non_blocking=False, copy=False, memory_format=None)\n",
      "  _4 = (_3).forward(input, )\n",
      "  _5 = (_2).forward(input0, )\n",
      "  _6 = (_1).forward(_4, )\n",
      "  _7 = (_0).forward(_5, )\n",
      "  u_feature = torch.relu(_6)\n",
      "  i_feature = torch.relu(_7)\n",
      "  batch_size = ops.prim.NumToTensor(torch.size(u_feature, 0))\n",
      "  _8 = int(batch_size)\n",
      "  u_feature0 = torch.reshape(u_feature, [int(batch_size), 1, 25])\n",
      "  i_feature0 = torch.reshape(i_feature, [_8, 25, 1])\n",
      "  output = torch.bmm(u_feature0, i_feature0)\n",
      "  return torch.squeeze(output)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(traced_model.code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scripted_model = torch.jit.script(nn_pure_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RecursiveScriptModule(\n",
       "  original_name=RecModelPureTorch\n",
       "  (UserEmbeddingLayer): RecursiveScriptModule(original_name=Embedding)\n",
       "  (ItemEmbeddingLayer): RecursiveScriptModule(original_name=Embedding)\n",
       "  (UserFeatureLayer): RecursiveScriptModule(original_name=Linear)\n",
       "  (ItemFeatureLayer): RecursiveScriptModule(original_name=Linear)\n",
       ")"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scripted_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def forward(self,\n",
      "    uid_batch: Tensor,\n",
      "    iid_batch: Tensor) -> Tensor:\n",
      "  _0 = self.ItemFeatureLayer\n",
      "  _1 = self.UserFeatureLayer\n",
      "  _2 = self.ItemEmbeddingLayer\n",
      "  _3 = self.UserEmbeddingLayer\n",
      "  input = torch.to(uid_batch, dtype=4, layout=0, device=torch.device(\"cpu\"), pin_memory=False, non_blocking=False, copy=False, memory_format=None)\n",
      "  input0 = torch.to(iid_batch, dtype=4, layout=0, device=torch.device(\"cpu\"), pin_memory=False, non_blocking=False, copy=False, memory_format=None)\n",
      "  _4 = (_3).forward(input, )\n",
      "  _5 = (_2).forward(input0, )\n",
      "  _6 = (_1).forward(_4, )\n",
      "  _7 = (_0).forward(_5, )\n",
      "  u_feature = torch.relu(_6)\n",
      "  i_feature = torch.relu(_7)\n",
      "  batch_size = ops.prim.NumToTensor(torch.size(u_feature, 0))\n",
      "  _8 = int(batch_size)\n",
      "  u_feature0 = torch.reshape(u_feature, [int(batch_size), 1, 25])\n",
      "  i_feature0 = torch.reshape(i_feature, [_8, 25, 1])\n",
      "  output = torch.bmm(u_feature0, i_feature0)\n",
      "  return torch.squeeze(output)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(traced_model.code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 简单模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseModel(torch.nn.Module):\n",
    "    def __init__(self,in_dim=1024,feature_size=256):\n",
    "        super(DenseModel,self).__init__()\n",
    "        self.Linear = torch.nn.Linear(in_features=in_dim,\n",
    "                                      out_features=feature_size)\n",
    "    def forward(self,x):\n",
    "        if x.shape[2]>512:\n",
    "            x = x[:,:,:512]\n",
    "        else:\n",
    "            x = x\n",
    "        out = self.Linear(x)\n",
    "        out = torch.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = DenseModel(in_dim=128,feature_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = torch.rand((64,32,128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 32, 64])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn(test_sample).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_dense = torch.jit.trace(nn,test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseModel(\n",
       "  original_name=DenseModel\n",
       "  (Linear): Linear(original_name=Linear)\n",
       ")"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def forward(self,\n",
      "    x: Tensor) -> Tensor:\n",
      "  _0 = torch.sigmoid((self.Linear).forward(x, ))\n",
      "  return _0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(trace_dense.code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "script_dense = torch.jit.script(nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RecursiveScriptModule(\n",
       "  original_name=DenseModel\n",
       "  (Linear): RecursiveScriptModule(original_name=Linear)\n",
       ")"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "script_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def forward(self,\n",
      "    x: Tensor) -> Tensor:\n",
      "  if torch.gt((torch.size(x))[2], 512):\n",
      "    _0 = torch.slice(x, 0, 0, 9223372036854775807, 1)\n",
      "    _1 = torch.slice(_0, 1, 0, 9223372036854775807, 1)\n",
      "    x0 = torch.slice(_1, 2, 0, 512, 1)\n",
      "  else:\n",
      "    x0 = x\n",
      "  out = (self.Linear).forward(x0, )\n",
      "  return torch.sigmoid(out)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(script_dense.code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trace_module = torch.jit.trace_module(nn,\n",
    "                      {'forward':test_sample})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseModel(\n",
       "  original_name=DenseModel\n",
       "  (Linear): Linear(original_name=Linear)\n",
       ")"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def forward(self,\n",
      "    x: Tensor) -> Tensor:\n",
      "  _0 = torch.sigmoid((self.Linear).forward(x, ))\n",
      "  return _0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(trace_module.code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
